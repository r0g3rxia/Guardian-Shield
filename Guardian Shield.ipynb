{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Import Necessary Libraries**\n",
    "\n",
    "    - Import various required libraries, including those for image processing, generating fake data, OCR, machine learning, etc.\n",
    "    - Initialize the `Faker` library, setting it to the UK locale to generate personal information in UK formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import easyocr\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import ttk\n",
    "from PIL import Image, ImageTk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Faker with UK locale\n",
    "fake = Faker('en_GB')  # Use the UK locale for Faker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Create Dataset Directories**\n",
    "\n",
    "   Create folders to store training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for datasets\n",
    "os.makedirs('data/train/images', exist_ok=True)\n",
    "os.makedirs('data/train/labels', exist_ok=True)\n",
    "os.makedirs('data/val/images', exist_ok=True)\n",
    "os.makedirs('data/val/labels', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Define Functions to Generate Personal Information**\n",
    "\n",
    "\n",
    "   Define functions to generate personal information (such as name, phone, email, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define types of PII\n",
    "pii_types = ['Name', 'Phone', 'Email', 'Passport', 'DriverLicense', 'PostalCode']\n",
    "\n",
    "# Define functions to generate UK-specific PII\n",
    "def generate_uk_phone_number():\n",
    "    return fake.phone_number()\n",
    "\n",
    "def generate_uk_email():\n",
    "    return fake.email()\n",
    "\n",
    "def generate_uk_passport_number():\n",
    "    return fake.bothify(text='########')\n",
    "\n",
    "def generate_uk_driver_license():\n",
    "    return fake.bothify(text='??????????')\n",
    "\n",
    "def generate_uk_postcode():\n",
    "    return fake.postcode()\n",
    "\n",
    "def generate_uk_name():\n",
    "    return fake.name()\n",
    "\n",
    "pii_funcs = [\n",
    "    generate_uk_name,\n",
    "    generate_uk_phone_number,\n",
    "    generate_uk_email,\n",
    "    generate_uk_passport_number,\n",
    "    generate_uk_driver_license,\n",
    "    generate_uk_postcode\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Generate Images with Personal Information**\n",
    "\n",
    "   - Generate images containing personal information and ensure that the information does not overlap within the image.\n",
    "   - Save the generated images and their corresponding annotation files (in XML format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_with_pii(image_path, label_path):\n",
    "    # Create a blank image with a white background\n",
    "    image = np.ones((500, 800, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    annotations = []\n",
    "    used_bboxes = []\n",
    "\n",
    "    def check_overlap(new_bbox):\n",
    "        for bbox in used_bboxes:\n",
    "            if (new_bbox[0] < bbox[2] and new_bbox[2] > bbox[0] and\n",
    "                new_bbox[1] < bbox[3] and new_bbox[3] > bbox[1]):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    for i, pii_func in enumerate(pii_funcs):\n",
    "        text = pii_func()\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = random.uniform(0.8, 1.2)  # Random font scale\n",
    "        thickness = random.randint(1, 2)  # Random thickness\n",
    "\n",
    "        text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "        attempts = 0\n",
    "        while attempts < 100:  # Try 100 times to find a non-overlapping position\n",
    "            text_x = random.randint(0, image.shape[1] - text_size[0])\n",
    "            text_y = random.randint(text_size[1], image.shape[0] - text_size[1])\n",
    "            new_bbox = [text_x, text_y - text_size[1], text_x + text_size[0], text_y]\n",
    "            \n",
    "            if not check_overlap(new_bbox):\n",
    "                used_bboxes.append(new_bbox)\n",
    "                break\n",
    "            attempts += 1\n",
    "        \n",
    "        if attempts == 100:\n",
    "            print(f\"Could not find non-overlapping position: {text}\")\n",
    "            continue\n",
    "\n",
    "        # Use black color for text to ensure visibility\n",
    "        color = (0, 0, 0)\n",
    "        # Draw text\n",
    "        cv2.putText(image, text, (text_x, text_y), font, font_scale, color, thickness)\n",
    "\n",
    "        annotations.append((pii_types[i], text, new_bbox))\n",
    "\n",
    "    # Save the image\n",
    "    cv2.imwrite(image_path, image)\n",
    "\n",
    "    # Save the annotation file\n",
    "    annotation = ET.Element('annotation')\n",
    "    ET.SubElement(annotation, 'folder').text = 'images'\n",
    "    ET.SubElement(annotation, 'filename').text = os.path.basename(image_path)\n",
    "    ET.SubElement(annotation, 'path').text = os.path.abspath(image_path)\n",
    "    \n",
    "    size = ET.SubElement(annotation, 'size')\n",
    "    ET.SubElement(size, 'width').text = str(image.shape[1])\n",
    "    ET.SubElement(size, 'height').text = str(image.shape[0])\n",
    "    ET.SubElement(size, 'depth').text = str(image.shape[2])\n",
    "\n",
    "    for (pii_type, text, bbox) in annotations:\n",
    "        obj = ET.SubElement(annotation, 'object')\n",
    "        ET.SubElement(obj, 'name').text = pii_type\n",
    "        ET.SubElement(obj, 'text').text = text  # Add text content\n",
    "        bndbox = ET.SubElement(obj, 'bndbox')\n",
    "        ET.SubElement(bndbox, 'xmin').text = str(bbox[0])\n",
    "        ET.SubElement(bndbox, 'ymin').text = str(bbox[1])\n",
    "        ET.SubElement(bndbox, 'xmax').text = str(bbox[2])\n",
    "        ET.SubElement(bndbox, 'ymax').text = str(bbox[3])\n",
    "\n",
    "    tree = ET.ElementTree(annotation)\n",
    "    tree.write(label_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Generate Training and Validation Datasets**\n",
    "\n",
    "\n",
    "   Generate 1000 training images and 200 validation images, saving their paths and annotation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and validation sets\n",
    "num_train = 1000\n",
    "num_val = 200\n",
    "\n",
    "for i in range(num_train):\n",
    "    image_path = f'data/train/images/img_{i}.jpg'\n",
    "    label_path = f'data/train/labels/img_{i}.xml'\n",
    "    create_image_with_pii(image_path, label_path)\n",
    "\n",
    "for i in range(num_val):\n",
    "    image_path = f'data/val/images/img_{i}.jpg'\n",
    "    label_path = f'data/val/labels/img_{i}.xml'\n",
    "    create_image_with_pii(image_path, label_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Initialize EasyOCR and Define OCR and Data Extraction Functions**\n",
    "\n",
    "\n",
    "   - Initialize EasyOCR and define functions to extract text information from images.\n",
    "   - Read text and labels from annotation files and match them with the OCR results to generate datasets for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "def ocr_image(image_path):\n",
    "    ocr_result = reader.readtext(image_path, detail=0)\n",
    "    return ocr_result\n",
    "\n",
    "def extract_labeled_texts(image_dir, label_dir):\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_path = os.path.join(image_dir, filename)\n",
    "            label_path = os.path.join(label_dir, filename.replace('.jpg', '.xml'))\n",
    "            \n",
    "            # Use OCR tool to extract text\n",
    "            ocr_result = ocr_image(image_path)\n",
    "            print(f\"OCR Result: {ocr_result}\")  # Print OCR result for debugging\n",
    "\n",
    "            # Read annotation file\n",
    "            tree = ET.parse(label_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            for obj in root.findall('object'):\n",
    "                name = obj.find('name').text\n",
    "                text = obj.find('text').text\n",
    "                for ocr_text in ocr_result:\n",
    "                    if ocr_text and ocr_text == text:\n",
    "                        data.append({'text': ocr_text, 'label': name})\n",
    "                        print(f\"Matched text: {ocr_text}, Label: {name}\")  # Print matched text and label for debugging\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Create Training and Validation Datasets**\n",
    "\n",
    "   Use OCR and annotation files to generate DataFrames for the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation datasets\n",
    "train_df = extract_labeled_texts('data/train/images', 'data/train/labels')\n",
    "val_df = extract_labeled_texts('data/val/images', 'data/val/labels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Check if All Required Labels are Present in the Dataset**\n",
    "\n",
    "   Check if the generated datasets contain all necessary labels and print any missing labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the data contains all required labels\n",
    "required_labels = ['Name', 'Phone', 'Email', 'Passport', 'DriverLicense', 'PostalCode']\n",
    "\n",
    "def check_labels(df, required_labels):\n",
    "    labels_in_data = df['label'].unique()\n",
    "    for label in required_labels:\n",
    "        if label not in labels_in_data:\n",
    "            print(f\"Warning: Missing label in dataset: {label}\")\n",
    "        else:\n",
    "            print(f\"Label {label} is present in the dataset\")\n",
    "\n",
    "check_labels(train_df, required_labels)\n",
    "check_labels(val_df, required_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **Text Preprocessing and Model Training**\n",
    "\n",
    "   - Preprocess the text by converting it to sequences and padding it.\n",
    "   - Use `LabelEncoder` to encode the labels.\n",
    "   - Build and train a neural network model with embedding, convolutional, and LSTM layers.\n",
    "   - Save the trained model and preprocessors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_df['text'].values)\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=100)\n",
    "    return padded_sequences\n",
    "\n",
    "X_train = preprocess_text(train_df['text'].values)\n",
    "X_val = preprocess_text(val_df['text'].values)\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df['label'].values)\n",
    "y_val = label_encoder.transform(val_df['label'].values)\n",
    "\n",
    "# Model training\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),  \n",
    "    Conv1D(256, 5, padding='same', activation='relu'),  \n",
    "    MaxPooling1D(pool_size=2),\n",
    "    LSTM(128),  \n",
    "    Dense(128, activation='relu'),  \n",
    "    Dropout(0.5),\n",
    "    Dense(len(set(train_df['label'])), activation='softmax') \n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))  \n",
    "\n",
    "# Save the model and preprocessors\n",
    "model.save('pii_model.h5')\n",
    "joblib.dump(tokenizer, 'tokenizer.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **Define OCR and Prediction Functions**\n",
    "\n",
    "This part of the process involves using EasyOCR to detect and extract text from input images, followed by predicting the type of Personal Identifiable Information (PII) using a pre-trained deep learning model:\n",
    "\n",
    " - OCR Execution: Utilizes EasyOCR to scan images for text, providing detailed results including the text content and its bounding box coordinates. This is essential for identifying textual elements within diverse and complex image backgrounds.\n",
    "\n",
    " - PII Classification: Each piece of extracted text is subsequently processed through a trained neural network model which categorizes it into specific PII types such as names, phone numbers, or email addresses. This classification is crucial for applications requiring data protection and compliance checks.\n",
    "\n",
    " - Visual Feedback: After classification, the original image is annotated with bounding boxes around each piece of text, color-coded by PII category. This visual augmentation helps users quickly understand the classification results and locate PII within the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and preprocessors\n",
    "model = load_model('pii_model.h5')\n",
    "tokenizer = joblib.load('tokenizer.pkl')\n",
    "label_encoder = joblib.load('label_encoder.pkl')\n",
    "\n",
    "# Initialize EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "def predict_pii(texts):\n",
    "    \"\"\"Predict the PII type for a list of texts using the trained model.\"\"\"\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    padded_seqs = pad_sequences(seqs, maxlen=100)\n",
    "    preds = model.predict(padded_seqs)\n",
    "    labels = label_encoder.inverse_transform(np.argmax(preds, axis=1))\n",
    "    return labels\n",
    "\n",
    "def ocr_image(image_path):\n",
    "    results = reader.readtext(image_path, detail=1)\n",
    "    merged_results = []\n",
    "\n",
    "    for result in results:\n",
    "        bbox, text, confidence = result\n",
    "        if len(merged_results) > 0 and is_adjacent(merged_results[-1][0], bbox):\n",
    "            merged_results[-1] = (\n",
    "                merge_bboxes(merged_results[-1][0], bbox),\n",
    "                merged_results[-1][1] + \" \" + text,\n",
    "                min(merged_results[-1][2], confidence)\n",
    "            )\n",
    "        else:\n",
    "            merged_results.append(result)\n",
    "\n",
    "    return merged_results\n",
    "\n",
    "def is_adjacent(bbox1, bbox2):\n",
    "    return abs(bbox1[2][0] - bbox2[0][0]) < 10\n",
    "\n",
    "def merge_bboxes(bbox1, bbox2):\n",
    "    x_min = min(bbox1[0][0], bbox2[0][0])\n",
    "    y_min = min(bbox1[0][1], bbox2[0][1])\n",
    "    x_max = max(bbox1[2][0], bbox2[2][0])\n",
    "    y_max = max(bbox1[2][1], bbox2[2][1])\n",
    "    return [[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]]\n",
    "\n",
    "def draw_boxes(image_path, ocr_results, labels):\n",
    "    \"\"\"Draw bounding boxes on the image based on OCR results and labels.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    color_dict = {\n",
    "        'Name': (0, 255, 255),  # Yellow\n",
    "        'Phone': (255, 0, 0),   # Blue\n",
    "        'Email': (0, 255, 255), # Yellow\n",
    "        'Passport': (0, 0, 255),# Red\n",
    "        'DriverLicense': (0, 0, 255), # Red\n",
    "        'PostalCode': (255, 0, 0)  # Blue\n",
    "    }\n",
    "    \n",
    "    for result, label in zip(ocr_results, labels):\n",
    "        top_left = tuple(result[0][0])\n",
    "        bottom_right = tuple(result[0][2])\n",
    "        text = result[1]\n",
    "        color = color_dict.get(label, (0, 255, 0))  # Default green\n",
    "        cv2.rectangle(image, top_left, bottom_right, color, 2)\n",
    "        text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "        cv2.rectangle(image, (top_left[0], top_left[1] - 30), (top_left[0] + text_size[0], top_left[1]), color, -1)\n",
    "        cv2.putText(image, label, (top_left[0], top_left[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. **Create Graphical User Interface (GUI)**\n",
    "\n",
    "This step is about creating a user-friendly interface using Tkinter, allowing users to easily interact with the image processing and PII detection functionalities:\n",
    "\n",
    " - GUI Setup: Develops a Tkinter-based GUI that facilitates the entire operation from image loading to displaying the processed results. This setup aims to make the application accessible and practical for everyday use.\n",
    "\n",
    " - Interactive Elements: Incorporates interactive elements such as a button to load images and a panel to display the annotated results. These elements are designed to provide a seamless user experience, enabling users to load and process images without any technical expertise.\n",
    "\n",
    " - Results Visualization: Displays the processed images directly in the GUI, with PII text highlighted and labeled according to its category. This immediate feedback allows users to effectively review and analyze the detected PII, enhancing usability in tasks requiring quick data verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the main application window\n",
    "root = tk.Tk()\n",
    "root.title(\"PII Detection Tool\")\n",
    "root.geometry(\"1000x700\")\n",
    "\n",
    "# Function to open an image file, process it, and display the results\n",
    "def open_image():\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png\")])\n",
    "    if file_path:\n",
    "        ocr_results = ocr_image(file_path)\n",
    "        texts = [result[1] for result in ocr_results]\n",
    "        labels = predict_pii(texts)\n",
    "        processed_image = draw_boxes(file_path, ocr_results, labels)\n",
    "        \n",
    "        # Convert image for display in Tkinter\n",
    "        processed_image = cv2.cvtColor(processed_image, cv2.COLOR_BGR2RGB)\n",
    "        processed_image = Image.fromarray(processed_image)\n",
    "        processed_image.thumbnail((800, 600))\n",
    "        img = ImageTk.PhotoImage(processed_image)\n",
    "        \n",
    "        # Display the image\n",
    "        panel.config(image=img)\n",
    "        panel.image = img\n",
    "\n",
    "# Create UI elements\n",
    "panel = tk.Label(root)\n",
    "panel.pack(padx=10, pady=10)\n",
    "\n",
    "btn = ttk.Button(root, text=\"Load Image\", command=open_image)\n",
    "btn.pack(pady=20)\n",
    "\n",
    "# Start the GUI event loop\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
